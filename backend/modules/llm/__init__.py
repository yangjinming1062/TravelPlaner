import json
import time
from collections.abc import AsyncGenerator
from typing import Any

from config import CONFIG
from langchain.schema import AIMessage
from langchain.schema import HumanMessage
from langchain.schema import SystemMessage
from langchain.tools import BaseTool
from pydantic import BaseModel
from utils import *

from .core import *
from .enums import *
from .exceptions import *
from .schemas import *


class LLMClient:
    """
    LangChainæ¨¡å‹é€‚é…å™¨ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œç»“æœè§£æ
    """

    def __init__(
        self,
        system_prompt: str = "",
        model_name: str = CONFIG.llm_model,
        temperature: float = CONFIG.llm_temperature,
        logger=None,
        max_history_size: int = 1000,
        max_concurrent_calls: int = 3,
        request_config: RequestConfig | None = None,
    ) -> None:
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨åˆ›å»ºï¼‰
            max_history_size: å†å²æ¶ˆæ¯æœ€å¤§æ•°é‡
            max_concurrent_calls: æœ€å¤§å¹¶å‘è°ƒç”¨æ•°ï¼ˆå·¥å…·ç®¡ç†å™¨ä½¿ç”¨ï¼‰
            request_config: RequestManager ç»Ÿä¸€é…ç½®ï¼ˆåŒ…å«æ¨¡å‹ã€é‡è¯•ã€å¹¶å‘ç­‰æ‰€æœ‰é…ç½®ï¼‰
        """
        self.system_prompt = system_prompt
        self.tools: list[BaseTool] = []
        self.logger = logger or get_logger("llm")
        self.metrics = MetricsCollector(self.logger)
        self.request_config = request_config or RequestConfig()
        self.model_name = model_name
        self.temperature = temperature
        self.request_manager = RequestManager(self.logger, self.metrics, self.request_config)
        self.history_manager = HistoryManager(self.logger, self.metrics, max_history_size=max_history_size, enable_validation=True, auto_cleanup=True)
        self.tool_manager = ToolManager(self.logger, self.metrics, max_concurrent_calls=max_concurrent_calls)
        self.chat_compressor = ChatCompressor(self.logger, self.metrics)
        self.model = create_llm_model(self.model_name, temperature=self.temperature)
        if self.system_prompt:
            system_msg = SystemMessage(content=self.system_prompt)
            self.history_manager.add_message(system_msg, MessageMetadata(timestamp=time.time(), model_name="system"))

    # region å·¥å…·ç®¡ç†

    def add_tools(self, tools: list[BaseTool]) -> None:
        """
        æ·»åŠ å·¥å…·åˆ°æ¨¡å‹

        Args:
            tools: å·¥å…·åˆ—è¡¨
        """
        if not self.tools:
            self.tools = []
        # æ‰©å±•å·¥å…·åˆ—è¡¨è€Œä¸æ˜¯æ›¿æ¢
        self.tools.extend(tools)
        # æ·»åŠ åˆ°å·¥å…·ç®¡ç†å™¨
        for tool in tools:
            self.tool_manager.register_tool(tool)
        # é‡æ–°ç»‘å®šæ‰€æœ‰å·¥å…·åˆ° LLMClient çš„æ¨¡å‹
        if self.tools and self.model:
            self.model = self.model.bind_tools(self.tools)
            self.logger.debug(f"å·²ä¸ºæ¨¡å‹ç»‘å®š {len(self.tools)} ä¸ªå·¥å…·")

    def get_tools(self) -> list[BaseTool]:
        """
        è·å–å·²æ³¨å†Œçš„å·¥å…·åˆ—è¡¨

        Returns:
            list[BaseTool]: å·¥å…·åˆ—è¡¨
        """
        return self.tools.copy() if self.tools else []

    def remove_tool(self, tool_name: str) -> bool:
        """
        ç§»é™¤æŒ‡å®šåç§°çš„å·¥å…·

        Args:
            tool_name: å·¥å…·åç§°

        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤
        """
        if not self.tools:
            return False

        for i, tool in enumerate(self.tools):
            if tool.name == tool_name:
                # é‡æ–°ç»‘å®šå‰©ä½™å·¥å…·åˆ° LLMClient çš„æ¨¡å‹
                if self.tools and self.model:
                    self.model = self.model.bind_tools(self.tools)
                elif self.model:
                    # å¦‚æœæ²¡æœ‰å·¥å…·äº†ï¼Œé‡æ–°åˆ›å»ºåŸå§‹æ¨¡å‹
                    self.model = create_llm_model(self.model_name, temperature=self.temperature)
                self.logger.debug(f"ç§»é™¤å·¥å…·: {tool_name}")
                return True

        self.logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
        return False

    def clear_tools(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰å·¥å…·"""
        tool_count = len(self.tools) if self.tools else 0
        self.tools = []
        # é‡æ–°åˆ›å»ºåŸå§‹æ¨¡å‹ï¼ˆæ— å·¥å…·ç»‘å®šï¼‰
        if self.model:
            self.model = create_llm_model(self.model_name, temperature=self.temperature)
        self.logger.debug(f"æ¸…ç©ºäº† {tool_count} ä¸ªå·¥å…·")

    def get_tool_by_name(self, tool_name: str) -> BaseTool | None:
        """
        æŒ‰åç§°è·å–å·¥å…·

        Args:
            tool_name: å·¥å…·åç§°

        Returns:
            BaseTool | None: æ‰¾åˆ°çš„å·¥å…·ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if not self.tools:
            return None

        for tool in self.tools:
            if tool.name == tool_name:
                return tool
        return None

    # endregion

    # region å†å²è®°å½•

    def get_history(self, mode: HistoryMode = HistoryMode.CURATED, limit: int | None = None, include_metadata: bool = False):
        """è·å–å¯¹è¯å†å²è®°å½•

        Args:
            mode: å†å²è®°å½•æ¨¡å¼ (CURATED/COMPREHENSIVE)
            limit: é™åˆ¶è¿”å›æ¡ç›®æ•°é‡
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®

        Returns:
            å†å²è®°å½•åˆ—è¡¨
        """
        return self.history_manager.get_history(mode=mode, limit=limit, include_metadata=include_metadata)

    def clear_history(self, keep_system_messages: bool = True):
        """æ¸…ç©ºå†å²è®°å½•

        Args:
            keep_system_messages: æ˜¯å¦ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        """
        self.history_manager.clear_history(keep_system_messages)
        self.logger.debug("å†å²è®°å½•å·²æ¸…ç©º")

    # endregion

    # region æ ¸å¿ƒäº¤äº’

    async def create_messages(self, user_prompt: str, use_history: bool = True):
        """
        åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆé›†æˆæ™ºèƒ½å‹ç¼©ï¼‰

        Args:
            user_prompt: ç”¨æˆ·æç¤º
            use_history: æ˜¯å¦ä½¿ç”¨å†å²è®°å½•

        Returns:
            åŒ…å«å†å²è®°å½•å’Œå½“å‰ç”¨æˆ·æç¤ºçš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []

        if use_history and self.history_manager:
            # è·å–æ›´å¤šå†å²è®°å½•ç”¨äºå‹ç¼©åˆ†æ
            if context_messages := self.history_manager.get_history(mode=HistoryMode.CURATED, include_metadata=False):
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
                should_compress = self.chat_compressor.should_compress(context_messages)
                if should_compress:
                    self.logger.info("è§¦å‘èŠå¤©å‹ç¼©ï¼šTokenæ•°é‡è¶…è¿‡é˜ˆå€¼")
                    try:
                        # æ‰§è¡Œå‹ç¼©
                        compression_result = await self.chat_compressor.compress_messages(context_messages, self.model_name)
                        # æ›´æ–°æŒ‡æ ‡
                        self.metrics.increment_metric("chat_compression", "total_compressions")
                        if compression_result.status == CompressionStatus.COMPRESSED:
                            # å‹ç¼©æˆåŠŸï¼Œä½¿ç”¨å‹ç¼©åçš„æ¶ˆæ¯
                            messages.extend(compression_result.compressed_messages)
                            # ğŸ”¥ æ›´æ–°å†å²è®°å½•ï¼Œé¿å…é‡å¤å‹ç¼©
                            self.history_manager.replace_with_compressed_messages(compression_result.compressed_messages)
                            self.metrics.increment_metric("chat_compression", "successful_compressions")
                            self.metrics.update_metric(
                                "chat_compression",
                                "total_tokens_saved",
                                compression_result.original_token_count - compression_result.compressed_token_count,
                            )
                            self.logger.debug(
                                f"èŠå¤©å‹ç¼©æˆåŠŸ: {compression_result.original_message_count}æ¡â†’{compression_result.compressed_message_count}æ¡æ¶ˆæ¯"
                            )
                        else:
                            # å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯çš„æˆªæ–­ç‰ˆæœ¬
                            messages.extend(context_messages[-20:])  # ä¿ç•™æœ€å20æ¡
                            self.metrics.increment_metric("chat_compression", "failed_compressions")
                            self.logger.warning(f"èŠå¤©å‹ç¼©å¤±è´¥: {compression_result.error_message}")
                    except Exception as e:
                        # å‹ç¼©è¿‡ç¨‹å‡ºé”™ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯
                        messages.extend(context_messages[-20:])
                        self.metrics.increment_metric("chat_compression", "failed_compressions")
                        self.logger.error(f"èŠå¤©å‹ç¼©å¼‚å¸¸: {e}")
                else:
                    # ä¸éœ€è¦å‹ç¼©ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                    messages.extend(context_messages[-20:])
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šåªæ·»åŠ ç³»ç»Ÿæç¤º
            if self.system_prompt:
                messages.append(SystemMessage(content=self.system_prompt))

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        user_message = HumanMessage(content=user_prompt)
        messages.append(user_message)

        return messages

    async def chat(
        self,
        user_prompt: str,
        config: dict = None,
        save_history: bool = True,
        stream: bool = False,
        response_format: dict | type[BaseModel] | None = None,
        **kwargs,
    ) -> AIMessage | AsyncGenerator | dict | BaseModel:
        """
        ç»Ÿä¸€çš„èŠå¤©æ–¹æ³• - æ”¯æŒæ‰€æœ‰ç±»å‹çš„è¯·æ±‚

        Args:
            user_prompt: ç”¨æˆ·è¾“å…¥
            config: æ¨¡å‹é…ç½®
            save_history: æ˜¯å¦ä¿å­˜åˆ°å†å²è®°å½•
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
            response_format: å¯é€‰çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼
            **kwargs: å…¶ä»–é…ç½®å‚æ•°

        Returns:
            æ ¹æ®å‚æ•°è¿”å›ä¸åŒç±»å‹ï¼š
            - æ ‡å‡†æ¨¡å¼: AIMessage
            - æµå¼æ¨¡å¼: AsyncGenerator
            - ç»“æ„åŒ–æ¨¡å¼: dict æˆ– BaseModel
        """
        # è¾“å…¥éªŒè¯
        if not user_prompt or not user_prompt.strip():
            raise ValueError("ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º")

        start_time = time.time()
        messages = await self.create_messages(user_prompt)
        self.metrics.increment_metric("global_metrics", "total_chat_requests")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
        if save_history:
            user_message = HumanMessage(content=user_prompt)
            self.history_manager.add_message(user_message, MessageMetadata(timestamp=start_time))

        try:
            self.logger.debug(f"å¼€å§‹æ‰§è¡ŒèŠå¤©è¯·æ±‚: stream={stream}, response_format={response_format is not None}")
            self.logger.debug(f"æ¶ˆæ¯æ•°é‡: {len(messages)}, æ¨¡å‹: {self.model_name}")
            # é€šè¿‡ RequestManager ç»Ÿä¸€çš„ request æ–¹æ³•è°ƒç”¨æ¨¡å‹
            if stream:
                # æµå¼è¯·æ±‚
                async def stream_model_call():
                    """æµå¼æ¨¡å‹è°ƒç”¨å‡½æ•°"""
                    self.logger.debug("æ‰§è¡Œæµå¼æ¨¡å‹è°ƒç”¨")
                    try:
                        if response_format:
                            # æµå¼ + ç»“æ„åŒ–è¾“å‡ºï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                            structured_model = self.model.with_structured_output(response_format)
                            result = structured_model.astream(messages, config or {})
                            self.logger.debug("æµå¼ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹è°ƒç”¨å®Œæˆ")
                            return result
                        else:
                            result = self.model.astream(messages, config or {})
                            self.logger.debug("æµå¼æ™®é€šæ¨¡å‹è°ƒç”¨å®Œæˆ")
                            return result
                    except Exception as e:
                        self.logger.exception(f"æµå¼æ¨¡å‹è°ƒç”¨å†…éƒ¨å¼‚å¸¸: {e}")
                        raise

                response = await self.request_manager.request(
                    stream_model_call,
                    priority=kwargs.get("priority", RequestPriority.NORMAL),
                    timeout=kwargs.get("timeout", 30.0),
                    metadata={"type": "stream", "user_prompt": user_prompt},
                )
            else:
                # æ ‡å‡†è¯·æ±‚
                async def standard_model_call():
                    """æ ‡å‡†æ¨¡å‹è°ƒç”¨å‡½æ•°"""
                    self.logger.debug("æ‰§è¡Œæ ‡å‡†æ¨¡å‹è°ƒç”¨")
                    try:
                        if response_format:
                            # ç»“æ„åŒ–è¾“å‡º
                            self.logger.debug(f"ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºæ ¼å¼: {response_format}")
                            structured_model = self.model.with_structured_output(response_format)
                            result = structured_model.invoke(messages, config or {})
                            self.logger.debug(f"æ ‡å‡†ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹è°ƒç”¨å®Œæˆ, å“åº”ç±»å‹: {type(result)}")
                            return result
                        else:
                            result = self.model.invoke(messages, config or {})
                            self.logger.debug(f"æ ‡å‡†æ™®é€šæ¨¡å‹è°ƒç”¨å®Œæˆ, å“åº”ç±»å‹: {type(result)}")
                            return result
                    except Exception as e:
                        self.logger.exception(f"æ ‡å‡†æ¨¡å‹è°ƒç”¨å†…éƒ¨å¼‚å¸¸: {e}")
                        raise

                self.logger.debug("é€šè¿‡RequestManageræäº¤æ ‡å‡†è¯·æ±‚")
                response = await self.request_manager.request(
                    standard_model_call,
                    priority=kwargs.get("priority", RequestPriority.NORMAL),
                    timeout=kwargs.get("timeout", 30.0),
                    metadata={"type": "standard", "user_prompt": user_prompt},
                )

            self.logger.debug(f"RequestManagerè¿”å›å“åº”: {type(response)}")

            # å¤„ç†æˆåŠŸå“åº”
            processing_time = time.time() - start_time
            # æ›´æ–°æˆåŠŸæŒ‡æ ‡
            self.metrics.increment_metric("global_metrics", "total_successful_chats")
            current_avg = self.metrics.global_metrics.get("average_response_time", 0.0)
            total_successful = self.metrics.global_metrics.get("total_successful_chats", 1)
            if total_successful > 0:
                new_avg = (current_avg * (total_successful - 1) + processing_time) / total_successful
                self.metrics.update_metric("global_metrics", "average_response_time", new_avg)

            # ç¡®ä¿å“åº”ä¸ä¸ºNone
            if response is None:
                raise ValueError("æ¨¡å‹è¿”å›äº†Noneå“åº”ï¼Œå¯èƒ½æ˜¯APIè°ƒç”¨å¤±è´¥")

            # ä¿å­˜å“åº”åˆ°å†å²è®°å½•ï¼ˆéæµå¼ï¼‰
            if save_history and not stream:
                response_metadata = MessageMetadata(timestamp=time.time(), model_name=self.model_name, processing_time=processing_time)

                # å¤„ç†ç»“æ„åŒ–è¾“å‡º - è½¬æ¢ä¸ºAIMessageä»¥ä¾¿å†å²è®°å½•å¤„ç†
                if response_format and not hasattr(response, "content"):
                    # ç»“æ„åŒ–è¾“å‡ºï¼šå°†å…¶è½¬æ¢ä¸ºAIMessage
                    if hasattr(response, "model_dump"):
                        # Pydanticæ¨¡å‹
                        content_str = json.dumps(response.model_dump(), ensure_ascii=False, indent=2)
                    elif isinstance(response, dict):
                        # å­—å…¸æ ¼å¼
                        content_str = json.dumps(response, ensure_ascii=False, indent=2)
                    else:
                        # å…¶ä»–ç±»å‹
                        content_str = str(response)

                    ai_message = AIMessage(content=f"[ç»“æ„åŒ–è¾“å‡º]\n{content_str}")
                    response_metadata.additional_data["structured_output"] = True
                    response_metadata.additional_data["original_format"] = str(type(response).__name__)
                    self.history_manager.add_message(ai_message, response_metadata)
                    self.logger.debug(f"ä¿å­˜ç»“æ„åŒ–è¾“å‡ºåˆ°å†å²è®°å½•: {type(response).__name__}")
                else:
                    # æ™®é€šå“åº”
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨å“åº”
                    if hasattr(response, "tool_calls") and response.tool_calls:
                        self.logger.debug(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {response.tool_calls}")
                        response_metadata.additional_data["has_tool_calls"] = True
                    self.history_manager.add_message(response, response_metadata)

            self.logger.debug(f"èŠå¤©è¯·æ±‚å®Œæˆï¼Œå¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            return response
        except Exception as e:
            self.logger.exception(f"èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
            self.logger.debug(f"å¼‚å¸¸è¯¦æƒ… - ç±»å‹: {type(e)}, æ¶ˆæ¯: {str(e)}")
            # æ›´æ–°å¤±è´¥æŒ‡æ ‡
            self.metrics.increment_metric("global_metrics", "total_failed_chats")
            # è®°å½•é”™è¯¯åˆ°å†å²
            if save_history:
                error_metadata = MessageMetadata(
                    timestamp=time.time(),
                    error_info={"error": str(e), "error_type": str(type(e))},
                    validation_status="invalid",
                )
                error_response = AIMessage(content="[ERROR: Request failed]")
                self.history_manager.add_message(error_response, error_metadata)
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸
            raise

    # endregion

    def get_system_status(self) -> dict[str, Any]:
        """è·å–ç³»ç»Ÿæ•´ä½“çŠ¶æ€ä¿¡æ¯"""
        return {
            "current_model": self.model_name,
            "tools_count": len(self.tools) if self.tools else 0,
            "system_prompt_length": len(self.system_prompt) if self.system_prompt else 0,
            "metrics": self.metrics.get_all_metrics(),
            "summary": self.metrics.get_summary(),
            "queue_info": self.request_manager.get_queue_info(),
        }


__all__ = ["LLMClient", "RequestConfig"]
