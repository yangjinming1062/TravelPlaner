#!/usr/bin/env python3
"""
LLMClient åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•åŠŸèƒ½åŒ…æ‹¬ï¼š
1. éæµå¼å“åº”
2. æµå¼å“åº”  
3. ç»“æ„åŒ–è¾“å‡º
4. è¿ç»­å¯¹è¯
5. å¯¹è¯å‹ç¼©
6. å·¥å…·è°ƒç”¨
7. é”™è¯¯å¤„ç†
8. ç³»ç»ŸçŠ¶æ€ç›‘æ§
"""
import asyncio
import json
import time

from modules.llm import LLMClient
from modules.llm import RequestConfig
from modules.planning import PlanningSingleResultSchema  # ç”¨äºç»“æ„åŒ–è¾“å‡ºçš„æµ‹è¯•æ¨¡å‹
from utils import get_logger

# å¯¼å…¥é¡¹ç›®æ¨¡å—

# é…ç½®æ—¥å¿—
logger = get_logger("test_llm")


class TestLLMClient:
    """LLMClient æµ‹è¯•ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.logger = logger
        self.test_results = {"passed": 0, "failed": 0, "errors": []}
        # åˆ›å»ºæµ‹è¯•é…ç½®
        self.config = RequestConfig(
            max_concurrent_requests=3,
            enable_auto_retry=True,
            max_retry_attempts=2,
            default_timeout=60.0,  # å¢åŠ é»˜è®¤è¶…æ—¶æ—¶é—´åˆ°60ç§’ï¼Œé€‚åº”å¤æ‚è¯·æ±‚
        )
        # åˆå§‹åŒ– LLMClient
        self.client = LLMClient(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜å’Œæä¾›å»ºè®®ã€‚", request_config=self.config, logger=self.logger
        )
        self.logger.info("ğŸš€ LLMClient æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

    def log_test_result(self, test_name: str, success: bool, details: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        if success:
            self.test_results["passed"] += 1
            self.logger.info(f"âœ… {test_name}: é€šè¿‡ {details}")
        else:
            self.test_results["failed"] += 1
            error_msg = f"âŒ {test_name}: å¤±è´¥ {details}"
            self.logger.error(error_msg)
            self.test_results["errors"].append(error_msg)

    async def test_basic_chat(self):
        """æµ‹è¯•åŸºæœ¬çš„éæµå¼å¯¹è¯"""
        self.logger.info("\nğŸ” æµ‹è¯• 1: åŸºæœ¬éæµå¼å¯¹è¯")
        try:
            response = await self.client.chat("è¯·ç®€å•ä»‹ç»ä¸€ä¸‹åŒ—äº¬çš„ä¸‰ä¸ªè‘—åæ™¯ç‚¹", save_history=True)
            success = isinstance(response, type(response)) and hasattr(response, "content") and len(response.content) > 50  # ç¡®ä¿æœ‰å®é™…å†…å®¹
            if success:
                self.log_test_result("åŸºæœ¬éæµå¼å¯¹è¯", True, f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
                self.logger.info(f"ğŸ“ å“åº”ç‰‡æ®µ: {response.content[:100]}...")
            else:
                self.log_test_result("åŸºæœ¬éæµå¼å¯¹è¯", False, "å“åº”æ ¼å¼æˆ–å†…å®¹å¼‚å¸¸")

        except Exception as e:
            self.log_test_result("åŸºæœ¬éæµå¼å¯¹è¯", False, f"å¼‚å¸¸: {str(e)}")

    async def test_streaming_chat(self):
        """æµ‹è¯•æµå¼å¯¹è¯"""
        self.logger.info("\nğŸ” æµ‹è¯• 2: æµå¼å¯¹è¯")
        try:
            response_generator = await self.client.chat("è¯·è®²ä¸€ä¸ªç®€çŸ­çš„ç«¥è¯æ•…äº‹", stream=True, save_history=True)

            chunks = []
            chunk_count = 0

            async for chunk in response_generator:
                chunk_count += 1
                if hasattr(chunk, "content") and chunk.content:
                    chunks.append(chunk.content)
                if chunk_count > 50:  # é˜²æ­¢æ— é™å¾ªç¯
                    break

            full_content = "".join(chunks)
            success = chunk_count > 0 and len(full_content) > 50

            if success:
                self.log_test_result("æµå¼å¯¹è¯", True, f"æ”¶åˆ° {chunk_count} ä¸ªå—ï¼Œæ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")
                self.logger.info(f"ğŸ“ æµå¼å†…å®¹ç‰‡æ®µ: {full_content[:100]}...")
            else:
                self.log_test_result("æµå¼å¯¹è¯", False, f"å—æ•°é‡ä¸è¶³æˆ–å†…å®¹è¿‡çŸ­")

        except Exception as e:
            self.log_test_result("æµå¼å¯¹è¯", False, f"å¼‚å¸¸: {str(e)}")

    async def test_structured_output(self):
        """æµ‹è¯•ç»“æ„åŒ–è¾“å‡º"""
        self.logger.info("\nğŸ” æµ‹è¯• 3: ç»“æ„åŒ–è¾“å‡º")

        try:
            # æµ‹è¯• Pydantic æ¨¡å‹è¾“å‡º
            response = await self.client.chat("è¯·æ¨èä¸€ä¸ªé€‚åˆæ˜¥å­£æ—…è¡Œçš„å›½å†…åŸå¸‚", response_format=PlanningSingleResultSchema, save_history=True)
            success = isinstance(response, PlanningSingleResultSchema)
            if success:
                self.log_test_result("ç»“æ„åŒ–è¾“å‡º(Pydantic)", True, f"æ ‡é¢˜: {response.plan_title}")
                self.logger.info(f"ğŸ“ ç»“æ„åŒ–å“åº”: {response.model_dump()}")
            else:
                self.log_test_result("ç»“æ„åŒ–è¾“å‡º(Pydantic)", False, f"å“åº”ç±»å‹: {type(response)}")

            # æµ‹è¯• JSON Schema è¾“å‡º
            json_schema = {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "åŸå¸‚åç§°"},
                    "temperature": {"type": "number", "description": "å½“å‰æ¸©åº¦"},
                    "weather": {"type": "string", "description": "å¤©æ°”çŠ¶å†µ"},
                },
                "required": ["city", "temperature", "weather"],
            }

            response2 = await self.client.chat(
                "å‘Šè¯‰æˆ‘ä¸Šæµ·ä»Šå¤©çš„å¤©æ°”æƒ…å†µï¼Œç”¨JSONæ ¼å¼å›ç­”", response_format=json_schema, save_history=False  # é¿å…å½±å“å¯¹è¯å†å²
            )

            json_success = isinstance(response2, dict) and "city" in response2

            if json_success:
                self.log_test_result("ç»“æ„åŒ–è¾“å‡º(JSON)", True, f"åŸå¸‚: {response2.get('city', 'N/A')}")
                self.logger.info(f"ğŸ“ JSONå“åº”: {response2}")
            else:
                self.log_test_result("ç»“æ„åŒ–è¾“å‡º(JSON)", False, f"å“åº”ç±»å‹: {type(response2)}")

        except Exception as e:
            self.log_test_result("ç»“æ„åŒ–è¾“å‡º", False, f"å¼‚å¸¸: {str(e)}")

    async def test_continuous_conversation(self):
        """æµ‹è¯•è¿ç»­å¯¹è¯èƒ½åŠ›"""
        self.logger.info("\nğŸ” æµ‹è¯• 4: è¿ç»­å¯¹è¯")

        try:
            # ç¬¬ä¸€è½®å¯¹è¯
            response1 = await self.client.chat("æˆ‘æ­£åœ¨è®¡åˆ’ä¸€æ¬¡åŒ—äº¬æ—…è¡Œï¼Œä½ æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ", save_history=True)

            # ç¬¬äºŒè½®å¯¹è¯ï¼ˆåº”è¯¥èƒ½è®°ä½ä¸Šä¸‹æ–‡ï¼‰
            response2 = await self.client.chat("é‚£ä½å®¿æ–¹é¢æœ‰ä»€ä¹ˆæ¨èå—ï¼Ÿ", save_history=True)

            # ç¬¬ä¸‰è½®å¯¹è¯
            response3 = await self.client.chat("é¢„ç®—å¤§æ¦‚éœ€è¦å¤šå°‘ï¼Ÿ", save_history=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            context_related = ("åŒ—äº¬" in response2.content or "ä½å®¿" in response2.content) and (
                "é¢„ç®—" in response3.content or "è´¹ç”¨" in response3.content or "é’±" in response3.content
            )

            success = all(
                [
                    hasattr(response1, "content") and len(response1.content) > 20,
                    hasattr(response2, "content") and len(response2.content) > 20,
                    hasattr(response3, "content") and len(response3.content) > 20,
                    context_related,
                ]
            )

            if success:
                self.log_test_result("è¿ç»­å¯¹è¯", True, "ä¸‰è½®å¯¹è¯éƒ½æœ‰åˆç†å“åº”ä¸”ä¿æŒä¸Šä¸‹æ–‡")
            else:
                self.log_test_result("è¿ç»­å¯¹è¯", False, "å¯¹è¯ç¼ºä¹ä¸Šä¸‹æ–‡è¿è´¯æ€§")

        except Exception as e:
            self.log_test_result("è¿ç»­å¯¹è¯", False, f"å¼‚å¸¸: {str(e)}")

    async def test_conversation_compression(self):
        """æµ‹è¯•å¯¹è¯å‹ç¼©åŠŸèƒ½"""
        self.logger.info("\nğŸ” æµ‹è¯• 5: å¯¹è¯å‹ç¼©")

        try:
            # ç”Ÿæˆå¤§é‡å¯¹è¯æ¥è§¦å‘å‹ç¼©
            questions = [
                "ä»‹ç»ä¸€ä¸‹æ•…å®«",
                "å¤©å›æœ‰ä»€ä¹ˆç‰¹è‰²ï¼Ÿ",
                "é•¿åŸçš„å†å²å¦‚ä½•ï¼Ÿ",
                "é¢å’Œå›­é€‚åˆä»€ä¹ˆæ—¶å€™å»ï¼Ÿ",
                "åŒ—æµ·å…¬å›­æœ‰ä»€ä¹ˆå¥½ç©çš„ï¼Ÿ",
                "é›å’Œå®«çš„å»ºç­‘é£æ ¼å¦‚ä½•ï¼Ÿ",
                "ä»€åˆ¹æµ·çš„å¤œæ™¯å¦‚ä½•ï¼Ÿ",
                "ç‹åºœäº•æœ‰ä»€ä¹ˆè´­ç‰©æ¨èï¼Ÿ",
            ]

            initial_history_size = len(self.client.history_manager.get_history())

            # è¿›è¡Œå¤šè½®å¯¹è¯
            for i, question in enumerate(questions):
                await self.client.chat(question, save_history=True)
                self.logger.info(f"å®Œæˆç¬¬ {i+1} è½®å¯¹è¯")

            final_history_size = len(self.client.history_manager.get_history())

            # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿäº†å‹ç¼©
            compression_occurred = final_history_size < len(questions) * 2  # æ¯è½®å¯¹è¯äº§ç”Ÿ2æ¡æ¶ˆæ¯

            # è·å–å‹ç¼©ç»Ÿè®¡
            compression_stats = self.client.metrics.get_all_metrics().get("chat_compression", {})
            total_compressions = compression_stats.get("total_compressions", 0)

            success = compression_occurred or total_compressions > 0

            if success:
                self.log_test_result("å¯¹è¯å‹ç¼©", True, f"å†å²å¤§å°: {initial_history_size} â†’ {final_history_size}, å‹ç¼©æ¬¡æ•°: {total_compressions}")
            else:
                self.log_test_result("å¯¹è¯å‹ç¼©", False, f"æœªæ£€æµ‹åˆ°å‹ç¼©ï¼Œå†å²å¤§å°: {final_history_size}")

        except Exception as e:
            self.log_test_result("å¯¹è¯å‹ç¼©", False, f"å¼‚å¸¸: {str(e)}")

    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        self.logger.info("\nğŸ” æµ‹è¯• 6: é”™è¯¯å¤„ç†")

        try:
            # æµ‹è¯•ç©ºè¾“å…¥
            try:
                await self.client.chat("", save_history=False)
                self.log_test_result("é”™è¯¯å¤„ç†(ç©ºè¾“å…¥)", False, "åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
            except Exception:
                self.log_test_result("é”™è¯¯å¤„ç†(ç©ºè¾“å…¥)", True, "æ­£ç¡®å¤„ç†ç©ºè¾“å…¥")

            # æµ‹è¯•æ— æ•ˆçš„ç»“æ„åŒ–æ ¼å¼
            try:
                # ä½¿ç”¨ä¸€ä¸ªçœŸæ­£æ— æ•ˆçš„æ ¼å¼ï¼ˆä¸æ˜¯å­—ç¬¦ä¸²ï¼‰
                invalid_format = {"invalid": "format", "without": "proper_schema"}
                await self.client.chat("æµ‹è¯•", response_format=invalid_format, save_history=False)  # æ— æ•ˆæ ¼å¼
                self.log_test_result("é”™è¯¯å¤„ç†(æ— æ•ˆæ ¼å¼)", False, "åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
            except Exception:
                self.log_test_result("é”™è¯¯å¤„ç†(æ— æ•ˆæ ¼å¼)", True, "æ­£ç¡®å¤„ç†æ— æ•ˆæ ¼å¼")

        except Exception as e:
            self.log_test_result("é”™è¯¯å¤„ç†", False, f"æµ‹è¯•å¼‚å¸¸: {str(e)}")

    async def test_system_monitoring(self):
        """æµ‹è¯•ç³»ç»ŸçŠ¶æ€ç›‘æ§"""
        self.logger.info("\nğŸ” æµ‹è¯• 7: ç³»ç»ŸçŠ¶æ€ç›‘æ§")

        try:
            # è·å–ç³»ç»ŸçŠ¶æ€
            status = self.client.get_system_status()

            # æ£€æŸ¥å¿…è¦çš„çŠ¶æ€é¡¹
            required_keys = ["current_model", "tools_count", "metrics", "queue_info"]
            has_required_keys = all(key in status for key in required_keys)

            # æ£€æŸ¥æŒ‡æ ‡æ•°æ®
            metrics = status.get("metrics", {})
            has_metrics = isinstance(metrics, dict) and len(metrics) > 0

            success = has_required_keys and has_metrics

            if success:
                self.log_test_result("ç³»ç»ŸçŠ¶æ€ç›‘æ§", True, f"çŠ¶æ€é¡¹: {len(status)}, æŒ‡æ ‡åˆ†ç±»: {len(metrics)}")
                self.logger.info(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {json.dumps(status, indent=2, ensure_ascii=False)}")
            else:
                self.log_test_result("ç³»ç»ŸçŠ¶æ€ç›‘æ§", False, "çŠ¶æ€ä¿¡æ¯ä¸å®Œæ•´")

        except Exception as e:
            self.log_test_result("ç³»ç»ŸçŠ¶æ€ç›‘æ§", False, f"å¼‚å¸¸: {str(e)}")

    def print_test_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        total = self.test_results["passed"] + self.test_results["failed"]
        success_rate = (self.test_results["passed"] / total * 100) if total > 0 else 0

        print("\n" + "=" * 60)
        print("ğŸ¯ LLMClient æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 60)
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {total}")
        print(f"âœ… é€šè¿‡: {self.test_results['passed']}")
        print(f"âŒ å¤±è´¥: {self.test_results['failed']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")

        if self.test_results["errors"]:
            print(f"\nâŒ å¤±è´¥è¯¦æƒ…:")
            for error in self.test_results["errors"]:
                print(f"  - {error}")

        print("\n" + "=" * 60)

        # æ‰“å°æœ€ç»ˆæŒ‡æ ‡
        try:
            final_metrics = self.client.metrics.get_summary()
            print("ğŸ“Š æœ€ç»ˆæŒ‡æ ‡æ€»ç»“:")
            for key, value in final_metrics.items():
                print(f"  - {key}: {value}")
        except Exception as e:
            print(f"è·å–æœ€ç»ˆæŒ‡æ ‡å¤±è´¥: {e}")

    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹ LLMClient å…¨é¢åŠŸèƒ½æµ‹è¯•")
        print("ğŸ” LLMClient åŠŸèƒ½æµ‹è¯•å¼€å§‹...")

        test_functions = [
            self.test_basic_chat,
            self.test_streaming_chat,
            self.test_structured_output,
            self.test_continuous_conversation,
            self.test_conversation_compression,
            self.test_error_handling,
            self.test_system_monitoring,
        ]

        start_time = time.time()

        for test_func in test_functions:
            try:
                await test_func()
            except Exception as e:
                self.logger.error(f"æµ‹è¯•å‡½æ•° {test_func.__name__} æ‰§è¡Œå¤±è´¥: {e}")

            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await asyncio.sleep(1)

        end_time = time.time()
        total_time = end_time - start_time

        self.logger.info(f"ğŸ æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’")
        self.print_test_summary()


async def main():
    """ä¸»å‡½æ•°"""
    tester = TestLLMClient()
    await tester.run_all_tests()

    # æ¸…ç†èµ„æº
    # await tester.client.shutdown()  # å¦‚æœæœ‰shutdownæ–¹æ³•çš„è¯


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
